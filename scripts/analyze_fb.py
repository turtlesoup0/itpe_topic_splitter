#!/usr/bin/env python3
"""
FBë°˜ ìë£Œ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
- ê¸°ì¶œ ì ì¤‘ë¥  ë¶„ì„ (FBë°˜ â†’ 137íšŒ/138íšŒ)
- ê³¼ëª©ë³„ í† í”½ ë¶„í¬
- ë¯¸ì¶œì œ í† í”½ ê´€ë¦¬
- ê¸°ìˆ˜ë³„ í•™ìŠµ ì§„í™” ë¶„ì„
- í•™ìŠµ ê°­ ë¶„ì„
"""
import json, os, re, sys
from collections import Counter, defaultdict
from datetime import datetime

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(os.path.dirname(SCRIPT_DIR), "data")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. ë°ì´í„° ë¡œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_json(name):
    path = os.path.join(DATA_DIR, name)
    with open(path, encoding="utf-8") as f:
        return json.load(f)


def load_all():
    topics = load_json("topics.json")
    e137 = load_json("exam137_report.json")
    e138 = load_json("exam138_report.json")
    return topics, e137, e138


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. ê¸°ì¶œ ë¬¸ì œ ì¤‘ë³µ ì œê±°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def dedup_exam_questions(exam_report):
    """sourceë³„ ì¤‘ë³µ ì œê±° â†’ (exam, session, q_num) ê¸°ì¤€ ê³ ìœ  ë¬¸ì œ ì¶”ì¶œ"""
    dedup = {}
    for r in exam_report["results"]:
        key = (r["exam"], r["session"], r["q_num"])
        if key not in dedup:
            dedup[key] = r["q_title"]
    return dedup


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. ë§¤ì¹­ ì—”ì§„
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# 137íšŒ ê³ ìœ  ë¬¸ì œë³„ í•µì‹¬ í‚¤ì›Œë“œ (ìˆ˜ë™ ì •ì˜ - ì •í™•ë„ ìœ„í•´)
EXAM_137_KEYWORDS = {
    # ê´€ 1êµì‹œ
    ("ê´€", 1, 1): {"terms": ["IGP", "EGP", "ë™ì ë¼ìš°íŒ…"], "label": "IGP/EGP ë™ì  ë¼ìš°íŒ… í”„ë¡œí† ì½œ"},
    ("ê´€", 1, 2): {"terms": ["ë””ì§€í„¸í¬ë Œì‹", "ì•„íŠ¸íŒ©íŠ¸", "FORENSIC"], "label": "ë””ì§€í„¸ í¬ë Œì‹ ì•„íŠ¸íŒ©íŠ¸"},
    ("ê´€", 1, 3): {"terms": ["MODBUS"], "label": "MODBUS í”„ë¡œí† ì½œ"},
    ("ê´€", 1, 4): {"terms": ["ì•”í˜¸ë¬¸ê³µê²©", "CIPHERTEXTATTACK"], "label": "ì•”í˜¸ë¬¸ ê³µê²©(Ciphertext Attack)"},
    ("ê´€", 1, 5): {"terms": ["GNN", "GRAPHNEURALNETWORK", "ê·¸ë˜í”„ì‹ ê²½ë§"], "label": "GNN(Graph Neural Network)"},
    ("ê´€", 1, 6): {"terms": ["AIê±°ë²„ë„ŒìŠ¤", "AIGOVERNANCE"], "label": "AI ê±°ë²„ë„ŒìŠ¤"},
    ("ê´€", 1, 7): {"terms": ["íŠ¸ëœìŠ¤í¬ë¨¸", "TRANSFORMER", "MOE", "MIXTUREOFEXPERTS"], "label": "íŠ¸ëœìŠ¤í¬ë¨¸/MoE"},
    ("ê´€", 1, 8): {"terms": ["AIì‹ ë¢°ì„±ê²€ì¸ì¦", "ì‹ ë¢°ì„±ê²€ì¦ì œë„"], "label": "AI ì‹ ë¢°ì„± ê²€ì¸ì¦ ì œë„(CAT)"},
    ("ê´€", 1, 9): {"terms": ["ABí…ŒìŠ¤íŒ…", "ABí…ŒìŠ¤íŠ¸", "ABTESTING"], "label": "A/B í…ŒìŠ¤íŒ…"},
    ("ê´€", 1, 10): {"terms": ["ë°ì´í„°ëŠª", "DATASWAMP"], "label": "ë°ì´í„° ëŠª(Data Swamp)"},
    ("ê´€", 1, 11): {"terms": ["ì—­ê³µí•™", "ì¬ê³µí•™", "REVERSEENGINEERING", "REENGINEERING"], "label": "ì†Œí”„íŠ¸ì›¨ì–´ ì—­ê³µí•™/ì¬ê³µí•™"},
    ("ê´€", 1, 12): {"terms": ["ì´ì§„íƒìƒ‰íŠ¸ë¦¬", "BINARYSEARCHTREE"], "label": "ì´ì§„ íƒìƒ‰ íŠ¸ë¦¬"},
    ("ê´€", 1, 13): {"terms": ["ì—°ê´€ê·œì¹™", "ASSOCIATIONRULE"], "label": "ë°ì´í„°ë§ˆì´ë‹ ì—°ê´€ ê·œì¹™ ë¶„ì„"},
    # ê´€ 2êµì‹œ
    ("ê´€", 2, 1): {"terms": ["ìºì‹œë©”ëª¨ë¦¬", "CACHEMEMORY", "ìºì‹œì¼ê´€ì„±", "CACHECOHERENCE"], "label": "ìºì‹œë©”ëª¨ë¦¬"},
    ("ê´€", 2, 2): {"terms": ["ìš´ì˜ì „í™˜", "ì „ììƒê±°ë˜"], "label": "ì „ììƒê±°ë˜ ì‹œìŠ¤í…œ ìš´ì˜ì „í™˜"},
    ("ê´€", 2, 3): {"terms": ["MCP", "MODELCONTEXTPROTOCOL"], "label": "MCP(Model Context Protocol) ë³´ì•ˆ"},
    ("ê´€", 2, 4): {"terms": ["ì´ˆê±°ëŒ€AI", "AIë„ì…ê°€ì´ë“œë¼ì¸"], "label": "ê³µê³µë¶€ë¬¸ ì´ˆê±°ëŒ€AI ë„ì… ê°€ì´ë“œë¼ì¸"},
    ("ê´€", 2, 5): {"terms": ["Q5"], "label": "2êµì‹œ Q5 (ì œëª© ë¯¸ì¶”ì¶œ)"},
    ("ê´€", 2, 6): {"terms": ["Q6"], "label": "2êµì‹œ Q6 (ì œëª© ë¯¸ì¶”ì¶œ)"},
    # ê´€ 3êµì‹œ
    ("ê´€", 3, 1): {"terms": ["ìŠ¤ì¼€ì¤„ë§ê¸°ë²•", "í”„ë¡œì„¸ìŠ¤ìŠ¤ì¼€ì¤„ë§"], "label": "ìš´ì˜ì²´ì œ ìŠ¤ì¼€ì¤„ë§ ê¸°ë²•"},
    ("ê´€", 3, 2): {"terms": ["ì •ë³´ì‹œìŠ¤í…œê°ë¦¬", "ìš´ì˜ê°ë¦¬", "ìœ ì§€ë³´ìˆ˜ê°ë¦¬"], "label": "ì •ë³´ì‹œìŠ¤í…œ ìš´ì˜/ìœ ì§€ë³´ìˆ˜ ê°ë¦¬"},
    ("ê´€", 3, 3): {"terms": ["MULTIREGION", "ë©€í‹°ë¦¬ì „", "ì¬í•´ë³µêµ¬ì‹œìŠ¤í…œ"], "label": "Multi-Region Active-Active ì¬í•´ë³µêµ¬"},
    ("ê´€", 3, 4): {"terms": ["Q4"], "label": "3êµì‹œ Q4 (ì œëª© ë¯¸ì¶”ì¶œ)"},
    ("ê´€", 3, 5): {"terms": ["Q5"], "label": "3êµì‹œ Q5 (ì œëª© ë¯¸ì¶”ì¶œ)"},
    ("ê´€", 3, 6): {"terms": ["Q6"], "label": "3êµì‹œ Q6 (ì œëª© ë¯¸ì¶”ì¶œ)"},
    # ê´€ 4êµì‹œ
    ("ê´€", 4, 1): {"terms": ["BPF", "BERKELEYPACKETFILTER"], "label": "BPF ì•…ì„±ì½”ë“œ"},
    ("ê´€", 4, 2): {"terms": ["ë²¡í„°ë°ì´í„°ë² ì´ìŠ¤", "HNSW", "VECTORDATABASE"], "label": "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤/HNSW"},
    ("ê´€", 4, 3): {"terms": ["ì¿ ë²„ë„¤í‹°ìŠ¤", "KUBERNETES", "K8S"], "label": "ì¿ ë²„ë„¤í‹°ìŠ¤(Kubernetes)"},
    ("ê´€", 4, 4): {"terms": ["UML", "í–‰ìœ„ë‹¤ì´ì–´ê·¸ë¨"], "label": "UML í–‰ìœ„ ë‹¤ì´ì–´ê·¸ë¨"},
    ("ê´€", 4, 5): {"terms": ["Q5"], "label": "4êµì‹œ Q5 (ì œëª© ë¯¸ì¶”ì¶œ)"},
    ("ê´€", 4, 6): {"terms": ["ëŒ€ê°€ì‚°ì •"], "label": "ì†Œí”„íŠ¸ì›¨ì–´ ì‚¬ì—… ëŒ€ê°€ì‚°ì •"},
}

# 138íšŒ ê³ ìœ  ë¬¸ì œë³„ í•µì‹¬ í‚¤ì›Œë“œ
EXAM_138_KEYWORDS = {
    ("ê´€", 1, 1): {"terms": ["AIRMF", "AIìœ„í—˜ê´€ë¦¬í”„ë ˆì„ì›Œí¬"], "label": "AI RMF(Risk Management Framework)"},
    ("ê´€", 1, 2): {"terms": ["í”„ë¡œì íŠ¸ìœ„í—˜ê´€ë¦¬", "ìœ„í—˜ê´€ë¦¬í”„ë¡œì„¸ìŠ¤"], "label": "í”„ë¡œì íŠ¸ ìœ„í—˜ê´€ë¦¬"},
    ("ê´€", 1, 3): {"terms": ["ISO42001", "IEC42001", "42001"], "label": "ISO/IEC 42001:2023"},
    ("ê´€", 1, 4): {"terms": ["ë² ì´ì¦ˆì •ë¦¬", "ë² ì´ì¦ˆ", "BAYES"], "label": "ë² ì´ì¦ˆ ì •ë¦¬"},
    ("ê´€", 1, 5): {"terms": ["ì•ˆë©´ì¸ì‹", "ì–¼êµ´ì¸ì‹ê²°ì œ"], "label": "ì•ˆë©´ì¸ì‹ ê²°ì œ ì„œë¹„ìŠ¤"},
    ("ê´€", 1, 6): {"terms": ["í…Œì¼ëŸ¬ë§", "TAILORING"], "label": "ê°œë°œë°©ë²•ë¡  í…Œì¼ëŸ¬ë§"},
    ("ê´€", 1, 7): {"terms": ["ìê¸°íšŒê·€ëª¨í˜•", "AUTOREGRESSIVE", "ì´ë™í‰ê· ëª¨í˜•", "ARIMA"], "label": "ìê¸°íšŒê·€ëª¨í˜•/ì´ë™í‰ê· ëª¨í˜•"},
    ("ê´€", 1, 8): {"terms": ["ì˜ì‚¬ê²°ì •ë‚˜ë¬´", "DECISIONTREE"], "label": "ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ ì˜ì‚¬ê²°ì •ë‚˜ë¬´"},
    ("ê´€", 1, 9): {"terms": ["ì œë¡œíŠ¸ëŸ¬ìŠ¤íŠ¸", "ZEROTRUST"], "label": "ì œë¡œ íŠ¸ëŸ¬ìŠ¤íŠ¸"},
    ("ê´€", 1, 10): {"terms": ["ê¸°ëŠ¥ì•ˆì „", "IEC61508", "FUNCTIONALSAFETY"], "label": "ê¸°ëŠ¥ì•ˆì „(IEC 61508)"},
    ("ê´€", 1, 11): {"terms": ["ì†Œí”„íŠ¸ì›¨ì–´ì •ì˜", "SDX", "SDV", "ì†Œí”„íŠ¸ì›¨ì–´ì •ì˜ê¸°ìˆ "], "label": "ì†Œí”„íŠ¸ì›¨ì–´ ì •ì˜ ê¸°ìˆ (SDx)"},
    ("ê´€", 1, 12): {"terms": ["ë””ì§€í„¸íŠ¸ìœˆ", "DIGITALTWIN"], "label": "ë””ì§€í„¸ íŠ¸ìœˆ"},
    ("ê´€", 1, 13): {"terms": ["CCPA", "GDPR", "ê°œì¸ì •ë³´ë³´í˜¸ë²•ë¹„êµ"], "label": "ê°œì¸ì •ë³´ë³´í˜¸ë²• ë¹„êµ (CCPA/GDPR)"},
    # 2êµì‹œ
    ("ê´€", 2, 1): {"terms": ["AIBOM", "AIBILLOFMATERIALS"], "label": "AI-BOM"},
    ("ê´€", 2, 2): {"terms": ["í˜•ìƒê´€ë¦¬", "CONFIGURATIONMANAGEMENT"], "label": "í˜•ìƒê´€ë¦¬"},
    ("ê´€", 2, 3): {"terms": ["CMMI", "CAPABILITYMATURITYMODEL"], "label": "CMMI 3.0"},
    ("ê´€", 2, 4): {"terms": ["ë°ì´í„°í’ˆì§ˆê´€ë¦¬", "ë°ì´í„°í’ˆì§ˆ"], "label": "ë°ì´í„° í’ˆì§ˆê´€ë¦¬"},
    ("ê´€", 2, 5): {"terms": ["ë©€í‹°í´ë¼ìš°ë“œ", "MULTICLOUD"], "label": "ë©€í‹° í´ë¼ìš°ë“œ"},
    ("ê´€", 2, 6): {"terms": ["ììœ¨ì£¼í–‰", "AUTONOMOUSDRIVING"], "label": "ììœ¨ì£¼í–‰"},
    # 3êµì‹œ
    ("ê´€", 3, 1): {"terms": ["SAAS", "SOFTWAREASASERVICE"], "label": "SaaS"},
    ("ê´€", 3, 2): {"terms": ["ë¸”ë¡ì²´ì¸", "BLOCKCHAIN"], "label": "ë¸”ë¡ì²´ì¸"},
    ("ê´€", 3, 3): {"terms": ["RAG", "RETRIEVALAUGMENTED"], "label": "RAG(ê²€ìƒ‰ ì¦ê°• ìƒì„±)"},
    ("ê´€", 3, 4): {"terms": ["ë¡œë“œë°¸ëŸ°ì‹±", "LOADBALANCING"], "label": "ë¡œë“œë°¸ëŸ°ì‹±"},
    ("ê´€", 3, 5): {"terms": ["í´ë¼ìš°ë“œë„¤ì´í‹°ë¸Œ", "CLOUDNATIVE"], "label": "ì „ìì •ë¶€ í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ"},
    ("ê´€", 3, 6): {"terms": ["OSPF", "BGP"], "label": "OSPF/BGP"},
    # 4êµì‹œ
    ("ê´€", 4, 1): {"terms": ["ISP", "ISMP"], "label": "ISP/ISMP"},
    ("ê´€", 4, 2): {"terms": ["AGENTICAI", "ì—ì´ì „í‹±"], "label": "Agentic AI"},
    ("ê´€", 4, 3): {"terms": ["ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤", "MSA", "MICROSERVICE"], "label": "ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤(MSA)"},
    ("ê´€", 4, 4): {"terms": ["ì–‘ìì»´í“¨íŒ…", "QUANTUMCOMPUTING", "ì–‘ì"], "label": "ì–‘ì ì»´í“¨íŒ…"},
    ("ê´€", 4, 5): {"terms": ["ì˜¨ë””ë°”ì´ìŠ¤AI", "ONDEVICEAI"], "label": "ì˜¨ë””ë°”ì´ìŠ¤ AI"},
    ("ê´€", 4, 6): {"terms": ["DEVSECOPS"], "label": "DevSecOps"},
}


def normalize(s):
    """í…ìŠ¤íŠ¸ ì •ê·œí™”: ê³µë°± ì œê±°, ëŒ€ë¬¸ìí™”"""
    s = re.sub(r"[\s\-_/Â·â€¢.,;:()ï¼ˆï¼‰ã€Œã€\[\]{}]", "", s)
    return s.upper()


def match_topic_to_exam(topics, exam_keywords, exam_num):
    """FB í† í”½ê³¼ ê¸°ì¶œ ë¬¸ì œ í‚¤ì›Œë“œ ë§¤ì¹­

    ë§¤ì¹­ ì¡°ê±´:
    - ì œëª©ì—ì„œ í‚¤ì›Œë“œ ë°œê²¬: +3ì /í‚¤ì›Œë“œ (ë†’ì€ ì‹ ë¢°ë„)
    - ë³¸ë¬¸ì—ì„œ í‚¤ì›Œë“œ ë°œê²¬: +1ì /í‚¤ì›Œë“œ (ë‚®ì€ ì‹ ë¢°ë„)
    - ìµœì†Œ ë§¤ì¹­ ê¸°ì¤€: ì œëª©ì—ì„œ 1ê°œ ì´ìƒ OR ë³¸ë¬¸ì—ì„œ 2ê°œ ì´ìƒ
    - ì¶œì œì˜ë„ ì°¸ì¡°ëŠ” ë³„ë„ í‘œì‹œ (ë§¤ì¹­ ì ìˆ˜ì— ë¶ˆí¬í•¨)
    """
    results = {}  # key â†’ list of matching topics

    for qkey, qinfo in exam_keywords.items():
        terms = qinfo["terms"]
        label = qinfo["label"]
        matches = []

        # Skip Q5/Q6 with no real title
        if all(t in ("Q5", "Q6", "Q4") for t in terms):
            results[qkey] = {"label": label, "matches": [], "skipped": True}
            continue

        for t in topics:
            search_title = normalize(t.get("q_title", ""))
            search_content = normalize(t.get("content", ""))

            # Method 1: ì¶œì œì˜ë„ì— íšŒì°¨ ì§ì ‘ ì–¸ê¸‰ (ë³„ë„ í‘œì‹œìš©)
            intent_match = False
            raw_intent = t.get("intent", "")
            if str(exam_num) in raw_intent and "íšŒ" in raw_intent:
                for m in re.findall(r"(\d{2,3})\s*(?:íšŒ|ê´€ë¦¬|ì‘ìš©|ì»´ì‹œì‘)", raw_intent):
                    if int(m) == exam_num:
                        intent_match = True
                        break

            # Method 2: í•µì‹¬ í‚¤ì›Œë“œ ë§¤ì¹­
            title_hits = 0
            content_hits = 0
            for term in terms:
                nterm = normalize(term)
                if len(nterm) < 2:
                    continue
                if nterm in search_title:
                    title_hits += 1
                elif nterm in search_content:
                    content_hits += 1

            # ì ìˆ˜ ì‚°ì¶œ: ì œëª© ë§¤ì¹­ = 3ì /ê±´, ë³¸ë¬¸ ë§¤ì¹­ = 1ì /ê±´
            score = title_hits * 3 + content_hits * 1

            # ìµœì†Œ ê¸°ì¤€: ì œëª©ì—ì„œ 1ê°œ ì´ìƒ OR ë³¸ë¬¸ì—ì„œ 2ê°œ ì´ìƒ
            is_valid = title_hits >= 1 or content_hits >= 2
            if not is_valid:
                continue

            matches.append({
                "gen": t["gen"],
                "week": t["week"],
                "title": t["q_title"][:60],
                "score": score,
                "intent_ref": intent_match,
                "title_hits": title_hits,
                "content_hits": content_hits,
            })

        # ì ìˆ˜ìˆœ ì •ë ¬, ìƒìœ„ 5ê°œ
        matches.sort(key=lambda x: -x["score"])
        results[qkey] = {"label": label, "matches": matches[:5], "skipped": False}

    return results


def extract_exam_refs_from_intent(topics):
    """ì¶œì œì˜ë„ì—ì„œ ê¸°ì¶œ íšŒì°¨ ë²ˆí˜¸ ì¶”ì¶œ"""
    exam_refs = defaultdict(list)  # exam_num â†’ list of topics
    for t in topics:
        intent = t.get("intent", "")
        for m in re.findall(r"(\d{2,3})\s*(?:íšŒ|ê´€ë¦¬|ì‘ìš©|ì»´ì‹œì‘)", intent):
            num = int(m)
            if 80 <= num <= 140:
                exam_refs[num].append({
                    "gen": t["gen"],
                    "week": t["week"],
                    "title": t["q_title"][:60],
                })
    return exam_refs


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. í†µê³„ ì‚°ì¶œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def subject_stats(topics):
    """ê³¼ëª©ë³„ í† í”½ ë¶„í¬"""
    by_subject = Counter()
    by_gen_subject = defaultdict(Counter)
    for t in topics:
        subj = t.get("subject", "UNKNOWN")
        gen = t["gen"]
        by_subject[subj] += 1
        by_gen_subject[gen][subj] += 1
    return by_subject, by_gen_subject


def unexamined_topics(topics):
    """ë¯¸ì¶œì œ í† í”½ ëª©ë¡"""
    result = []
    for t in topics:
        intent = t.get("intent", "")
        if "ë¯¸ì¶œì œ" in intent:
            result.append({
                "gen": t["gen"],
                "week": t["week"],
                "subject": t.get("subject", "UNKNOWN"),
                "title": t["q_title"][:60],
                "intent": intent[:100],
            })
    return result


def gen_stats(topics):
    """ê¸°ìˆ˜ë³„ í†µê³„"""
    by_gen = Counter()
    by_gen_week = defaultdict(set)
    by_gen_session = defaultdict(Counter)
    for t in topics:
        gen = t["gen"]
        by_gen[gen] += 1
        by_gen_week[gen].add(t["week"])
        sess = t.get("session", "UNKNOWN")
        by_gen_session[gen][sess] += 1
    return by_gen, by_gen_week, by_gen_session


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def generate_report(topics, e137, e138):
    lines = []
    now = datetime.now().strftime("%Y-%m-%d %H:%M")

    # â”€â”€ Header â”€â”€
    lines.append("---")
    lines.append("title: FBë°˜ ìë£Œ ë¶„ì„ ë¦¬í¬íŠ¸")
    lines.append(f"date: {now}")
    lines.append("tags: [ë¶„ì„, FBë°˜, ê¸°ì¶œ, ì ì¤‘ë¥ ]")
    lines.append("---")
    lines.append("")
    lines.append("# FBë°˜ ìë£Œ ë¶„ì„ ë¦¬í¬íŠ¸")
    lines.append(f"> ìƒì„±ì¼: {now}")
    lines.append("")

    # â”€â”€ 1. ìš”ì•½ â”€â”€
    lines.append("## 1. ì „ì²´ ìš”ì•½")
    lines.append("")

    by_gen, by_gen_week, by_gen_session = gen_stats(topics)
    by_subject, by_gen_subject = subject_stats(topics)
    unexam = unexamined_topics(topics)

    lines.append(f"| í•­ëª© | ê°’ |")
    lines.append(f"|---|---|")
    lines.append(f"| ì´ í† í”½ ìˆ˜ | **{len(topics)}**ê°œ |")
    for g in sorted(by_gen.keys()):
        lines.append(f"| {g} | {by_gen[g]}ê°œ ({len(by_gen_week[g])}ì£¼ì°¨) |")
    lines.append(f"| ì¶œì œì˜ë„ ìˆìŒ | {sum(1 for t in topics if t.get('intent','').strip())}ê°œ |")
    lines.append(f"| ì‘ì„±ë°©ì•ˆ ìˆìŒ | {sum(1 for t in topics if t.get('approach','').strip())}ê°œ |")
    lines.append(f"| ë³¸ë¬¸ ìˆìŒ | {sum(1 for t in topics if t.get('content','').strip())}ê°œ |")
    lines.append(f"| ë¯¸ì¶œì œ í† í”½ | {len(unexam)}ê°œ |")
    lines.append("")

    # â”€â”€ 2. ê³¼ëª©ë³„ ë¶„í¬ â”€â”€
    lines.append("## 2. ê³¼ëª©ë³„ í† í”½ ë¶„í¬")
    lines.append("")
    lines.append("| ê³¼ëª© | ì „ì²´ | 19ê¸° | 20ê¸° | 21ê¸° |")
    lines.append("|---|---|---|---|---|")
    for subj, cnt in sorted(by_subject.items(), key=lambda x: -x[1]):
        c19 = by_gen_subject.get("19ê¸°", {}).get(subj, 0)
        c20 = by_gen_subject.get("20ê¸°", {}).get(subj, 0)
        c21 = by_gen_subject.get("21ê¸°", {}).get(subj, 0)
        bar = "â–ˆ" * (cnt // 5) + "â–‘" * max(0, 10 - cnt // 5)
        lines.append(f"| {subj} | **{cnt}** {bar} | {c19} | {c20} | {c21} |")
    lines.append("")

    # â”€â”€ 3. ê¸°ìˆ˜ë³„ ë¹„êµ â”€â”€
    lines.append("## 3. ê¸°ìˆ˜ë³„ í•™ìŠµ ì§„í™” ë¶„ì„")
    lines.append("")
    for g in sorted(by_gen.keys()):
        lines.append(f"### {g} ({by_gen[g]}ê°œ í† í”½, {len(by_gen_week[g])}ì£¼ì°¨)")
        sess_str = ", ".join(f"{s}:{c}" for s, c in sorted(by_gen_session[g].items()))
        lines.append(f"- êµì‹œ ë¶„í¬: {sess_str}")
        subj_str = ", ".join(f"{s}:{c}" for s, c in sorted(by_gen_subject[g].items(), key=lambda x: -x[1]))
        lines.append(f"- ê³¼ëª© ë¶„í¬: {subj_str}")
        lines.append("")

    # â”€â”€ 4. ê¸°ì¶œ ì ì¤‘ë¥  ë¶„ì„ â”€â”€
    lines.append("## 4. ê¸°ì¶œ ì ì¤‘ë¥  ë¶„ì„")
    lines.append("")

    # Helper: render match table for an exam
    def render_match_table(match_result, exam_num):
        """ì ì¤‘ë¥  í…Œì´ë¸” ë Œë”ë§ (3ë‹¨ê³„: âœ…í™•ì‹¤/ğŸŸ¡ê°„ì ‘/âŒë¯¸ì»¤ë²„)"""
        direct = 0   # ì œëª©ì—ì„œ í‚¤ì›Œë“œ ë°œê²¬
        indirect = 0  # ë³¸ë¬¸ì—ì„œë§Œ í‚¤ì›Œë“œ ë°œê²¬
        missed = 0
        scorable = 0

        table_lines = []
        table_lines.append("| êµì‹œ | ë¬¸ë²ˆ | ê¸°ì¶œ í† í”½ | ë§¤ì¹­ | FBë°˜ í† í”½ (ìµœê³  ë§¤ì¹­) |")
        table_lines.append("|---|---|---|---|---|")

        for qkey in sorted(match_result.keys()):
            v = match_result[qkey]
            exam, sess, qnum = qkey
            if v.get("skipped"):
                table_lines.append(f"| {sess}êµì‹œ | Q{qnum:02d} | {v['label']} | â­ï¸ ë¯¸ì¶”ì¶œ | - |")
                continue

            scorable += 1
            if v["matches"]:
                best = v["matches"][0]
                ref_icon = "ğŸ“Œ" if best.get("intent_ref") else ""
                if best.get("title_hits", 0) >= 1:
                    # ì œëª©ì—ì„œ í‚¤ì›Œë“œ ë°œê²¬ = í™•ì‹¤í•œ ë§¤ì¹­
                    direct += 1
                    table_lines.append(f"| {sess}êµì‹œ | Q{qnum:02d} | {v['label']} | âœ… í™•ì‹¤ | [{best['gen']}] {best['title']} {ref_icon} |")
                else:
                    # ë³¸ë¬¸ì—ì„œë§Œ ë°œê²¬ = ê°„ì ‘ ë§¤ì¹­ (content ë²ˆë“¤ë§ ê°€ëŠ¥ì„±)
                    indirect += 1
                    table_lines.append(f"| {sess}êµì‹œ | Q{qnum:02d} | {v['label']} | ğŸŸ¡ ê°„ì ‘ | [{best['gen']}] {best['title']} {ref_icon} |")
            else:
                missed += 1
                table_lines.append(f"| {sess}êµì‹œ | Q{qnum:02d} | {v['label']} | âŒ | *ë¯¸ì»¤ë²„* |")

        return table_lines, direct, indirect, missed, scorable

    # 4-1. 137íšŒ
    match_137 = match_topic_to_exam(topics, EXAM_137_KEYWORDS, 137)
    tbl_137, d137, i137, m137, s137 = render_match_table(match_137, 137)
    lines.append(f"### 4-1. 137íšŒ ì ì¤‘ë¥ ")
    lines.append("")
    lines.append(f"| êµ¬ë¶„ | ìˆ˜ | ë¹„ìœ¨ |")
    lines.append(f"|---|---|---|")
    lines.append(f"| âœ… í™•ì‹¤ (ì œëª© ë§¤ì¹­) | {d137} | {d137*100//max(s137,1)}% |")
    lines.append(f"| ğŸŸ¡ ê°„ì ‘ (ë³¸ë¬¸ ë§¤ì¹­) | {i137} | {i137*100//max(s137,1)}% |")
    lines.append(f"| âŒ ë¯¸ì»¤ë²„ | {m137} | {m137*100//max(s137,1)}% |")
    lines.append(f"| â­ï¸ ë¯¸ì¶”ì¶œ | {sum(1 for v in match_137.values() if v.get('skipped'))} | - |")
    lines.append(f"| **ì´ ì ì¤‘** | **{d137+i137}/{s137}** | **{(d137+i137)*100//max(s137,1)}%** |")
    lines.append("")
    lines.extend(tbl_137)
    lines.append("")
    lines.append("> âœ… í™•ì‹¤ = FB í† í”½ ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì§ì ‘ ë°œê²¬")
    lines.append("> ğŸŸ¡ ê°„ì ‘ = FB í† í”½ ë³¸ë¬¸ì—ì„œë§Œ ë°œê²¬ (ê°™ì€ ë¦¬ë·° ì„¸ì…˜ì— í¬í•¨ëœ ë‹¤ë¥¸ í† í”½ì¼ ìˆ˜ ìˆìŒ)")
    lines.append("> ğŸ“Œ = ì¶œì œì˜ë„ì—ì„œ í•´ë‹¹ íšŒì°¨ ì§ì ‘ ì–¸ê¸‰")
    lines.append("")

    # 4-2. 138íšŒ
    match_138 = match_topic_to_exam(topics, EXAM_138_KEYWORDS, 138)
    tbl_138, d138, i138, m138, s138 = render_match_table(match_138, 138)
    lines.append(f"### 4-2. 138íšŒ ì ì¤‘ë¥ ")
    lines.append("")
    lines.append(f"| êµ¬ë¶„ | ìˆ˜ | ë¹„ìœ¨ |")
    lines.append(f"|---|---|---|")
    lines.append(f"| âœ… í™•ì‹¤ (ì œëª© ë§¤ì¹­) | {d138} | {d138*100//max(s138,1)}% |")
    lines.append(f"| ğŸŸ¡ ê°„ì ‘ (ë³¸ë¬¸ ë§¤ì¹­) | {i138} | {i138*100//max(s138,1)}% |")
    lines.append(f"| âŒ ë¯¸ì»¤ë²„ | {m138} | {m138*100//max(s138,1)}% |")
    lines.append(f"| **ì´ ì ì¤‘** | **{d138+i138}/{s138}** | **{(d138+i138)*100//max(s138,1)}%** |")
    lines.append("")
    lines.extend(tbl_138)
    lines.append("")

    # â”€â”€ 5. í•™ìŠµ ê°­ ë¶„ì„ â”€â”€
    lines.append("## 5. í•™ìŠµ ê°­ ë¶„ì„")
    lines.append("")
    lines.append("### 5-1. 137íšŒ ê¸°ì¶œ ì¤‘ FBë°˜ ë¯¸ì»¤ë²„ í† í”½")
    lines.append("")
    gap_137 = [(k, v) for k, v in sorted(match_137.items())
               if not v["matches"] and not v.get("skipped")]
    if gap_137:
        for qkey, v in gap_137:
            lines.append(f"- **{qkey[1]}êµì‹œ Q{qkey[2]:02d}**: {v['label']}")
    else:
        lines.append("- ì—†ìŒ (ëª¨ë“  ì¶”ì¶œ ë¬¸ì œ ì»¤ë²„)")
    lines.append("")

    lines.append("### 5-2. 138íšŒ ê¸°ì¶œ ì¤‘ FBë°˜ ë¯¸ì»¤ë²„ í† í”½")
    lines.append("")
    gap_138 = [(k, v) for k, v in sorted(match_138.items())
               if not v["matches"] and not v.get("skipped")]
    if gap_138:
        for qkey, v in gap_138:
            lines.append(f"- **{qkey[1]}êµì‹œ Q{qkey[2]:02d}**: {v['label']}")
    else:
        lines.append("- ì—†ìŒ (ëª¨ë“  ì¶”ì¶œ ë¬¸ì œ ì»¤ë²„)")
    lines.append("")

    # â”€â”€ 6. ë¯¸ì¶œì œ í† í”½ â”€â”€
    lines.append("## 6. ë¯¸ì¶œì œ í† í”½ ëª©ë¡ (í–¥í›„ ì¶œì œ ëŒ€ë¹„)")
    lines.append("")
    lines.append(f"> ì¶œì œì˜ë„ì— 'ë¯¸ì¶œì œ'ë¡œ ëª…ì‹œëœ **{len(unexam)}ê°œ** í† í”½")
    lines.append("")
    lines.append("| # | ê¸°ìˆ˜ | ì£¼ì°¨ | ê³¼ëª© | í† í”½ëª… |")
    lines.append("|---|---|---|---|---|")
    for i, u in enumerate(unexam, 1):
        lines.append(f"| {i} | {u['gen']} | {u['week']} | {u['subject']} | {u['title']} |")
    lines.append("")

    # â”€â”€ 7. ê¸°ì¶œ íšŒì°¨ë³„ FBë°˜ ì—°ê´€ í† í”½ â”€â”€
    lines.append("## 7. ê¸°ì¶œ íšŒì°¨ë³„ FBë°˜ ì°¸ì¡° í˜„í™©")
    lines.append("")
    exam_refs = extract_exam_refs_from_intent(topics)
    lines.append("| íšŒì°¨ | ì°¸ì¡° í† í”½ ìˆ˜ |")
    lines.append("|---|---|")
    for exam_num in sorted(exam_refs.keys(), reverse=True):
        if exam_num >= 100:
            refs = exam_refs[exam_num]
            lines.append(f"| {exam_num}íšŒ | {len(refs)} |")
    lines.append("")

    # Detail for 137
    if 137 in exam_refs:
        lines.append("### 137íšŒ ì§ì ‘ ì°¸ì¡° í† í”½")
        lines.append("")
        for ref in exam_refs[137]:
            lines.append(f"- [{ref['gen']}] {ref['title']}")
        lines.append("")

    # â”€â”€ 8. í•™ìŠµ ì¶”ì²œ â”€â”€
    lines.append("## 8. í•™ìŠµ ì¶”ì²œ")
    lines.append("")

    # ë¯¸ì»¤ë²„ ê°­ ì¶”ì²œ
    all_gaps = gap_137 + gap_138
    if all_gaps:
        lines.append("### ğŸ”´ ìš°ì„  ë³´ê°• í•„ìš” (ê¸°ì¶œ ë¯¸ì»¤ë²„)")
        lines.append("")
        for qkey, v in all_gaps:
            lines.append(f"- {v['label']}")
        lines.append("")

    # ë¯¸ì¶œì œ ìµœì‹  íŠ¸ë Œë“œ
    lines.append("### ğŸŸ¡ ë¯¸ì¶œì œ ìµœì‹  í† í”½ (ì¶œì œ ì˜ˆìƒ)")
    lines.append("")
    trend_keywords = ["ê°€íŠ¸ë„ˆ", "AI", "í´ë¼ìš°ë“œ", "ë³´ì•ˆ", "ì–‘ì", "ë¸”ë¡ì²´ì¸", "6G"]
    for u in unexam:
        if any(kw in u["title"] or kw in u.get("intent", "") for kw in trend_keywords):
            lines.append(f"- [{u['gen']}] {u['title']}")
    lines.append("")

    # ê³ ë¹ˆë„ ì¶œì œ ê³¼ëª©
    lines.append("### ğŸŸ¢ ê³ ë¹ˆë„ ê³¼ëª© (ì¶©ë¶„í•œ í•™ìŠµëŸ‰)")
    lines.append("")
    for subj, cnt in sorted(by_subject.items(), key=lambda x: -x[1])[:5]:
        lines.append(f"- {subj}: {cnt}ê°œ í† í”½")
    lines.append("")

    return "\n".join(lines)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Main
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    print("FBë°˜ ìë£Œ ë¶„ì„ ì‹œì‘...")
    topics, e137, e138 = load_all()
    print(f"  í† í”½: {len(topics)}ê°œ, 137íšŒ: {len(e137['results'])}ê°œ, 138íšŒ: {len(e138['results'])}ê°œ")

    report = generate_report(topics, e137, e138)

    out_path = os.path.join(DATA_DIR, "fb_analysis_report.md")
    with open(out_path, "w", encoding="utf-8") as f:
        f.write(report)
    print(f"  ë¦¬í¬íŠ¸ ìƒì„±: {out_path}")
    print(f"  íŒŒì¼ í¬ê¸°: {os.path.getsize(out_path):,} bytes")

    # Also print summary to console
    print("\n" + "=" * 60)
    print("ìš”ì•½")
    print("=" * 60)
    match_137 = match_topic_to_exam(topics, EXAM_137_KEYWORDS, 137)
    match_138 = match_topic_to_exam(topics, EXAM_138_KEYWORDS, 138)

    def count_matches(match_result):
        direct = indirect = missed = scorable = 0
        for v in match_result.values():
            if v.get("skipped"):
                continue
            scorable += 1
            if v["matches"]:
                if v["matches"][0].get("title_hits", 0) >= 1:
                    direct += 1
                else:
                    indirect += 1
            else:
                missed += 1
        return direct, indirect, missed, scorable

    d137, i137, m137, s137 = count_matches(match_137)
    d138, i138, m138, s138 = count_matches(match_138)
    print(f"  137íšŒ: í™•ì‹¤ {d137} + ê°„ì ‘ {i137} = {d137+i137}/{s137} ({(d137+i137)*100//max(s137,1)}%), ë¯¸ì»¤ë²„ {m137}")
    print(f"  138íšŒ: í™•ì‹¤ {d138} + ê°„ì ‘ {i138} = {d138+i138}/{s138} ({(d138+i138)*100//max(s138,1)}%), ë¯¸ì»¤ë²„ {m138}")
    print(f"  ë¯¸ì¶œì œ í† í”½: {len(unexamined_topics(topics))}ê°œ")
    print(f"  ê³¼ëª© ìˆ˜: {len(set(t.get('subject','') for t in topics))}ê°œ")


if __name__ == "__main__":
    main()
